{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Chunk-Size for Large Document Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Large Language Model (LLM) is good at text summarization. However, the limited context window of LLM poses a challenge when summarizing large documents. We discuss a limitation of the commonly used summarization strategy and introduce a straightforward improvement.\n",
    "\n",
    "*Source*: [Optimal Chunk-Size for Large Document Summarization](https://vectify.ai/blog/LargeDocumentSummarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "OPENAI_API_TYPE = \"YOUR_API_TYPE\"\n",
    "OPENAI_API_VERSION = \"YOUR_API_VERSION\"\n",
    "AZURE_OPENAI_ENDPOINT = \"YOUR_ENDPOINT\"\n",
    "AZURE_OPENAI_LLM_DEPLOYMENT_NAME = \"YOUR_LLM_DEPLOYMENT_NAME\"\n",
    "AZURE_OPENAI_LLM_MODEL_NAME = \"YOUR_LLM_MODEL_NAME\"\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = \"YOUR_EMBEDDING_DEPLOYMENT_NAME\"\n",
    "AZURE_OPENAI_EMBEDDING_MODEL_NAME = \"YOUR_EMBEDDING_MODEL_NAME\"\n",
    "OPENAI_API_KEY = \"YOUR_API_KEY\"\n",
    "\n",
    "CONFLUENCE_TOKEN = \"YOUR_CONFLUENCE_TOKEN\"\n",
    "\n",
    "COSMOSDB_VCORE_CONNECTION_STRING = \"YOUR_COSMOSDB_VCORE_CONNECTION_STRING\"\n",
    "COSMOSDB_NAMESPACE = \"YOUR_COSMOSDB_NAMESPACE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ConfluenceLoader\n",
    "import pytesseract\n",
    "\n",
    "# This section uses internal Confluence documents and cannot be shared.\n",
    "# Replace this loader with your own Confluence space or another document source.\n",
    "\n",
    "loader = ConfluenceLoader(\n",
    "    url=\"https://your-confluence-domain.com\"\n",
    "    token=\"YOUR_CONFLUENCE_TOKEN\"\n",
    ")\n",
    "\n",
    "confluence_documents = loader.load(\n",
    "    space_key=\"EXAMPLE_SPACE\", \n",
    "    include_attachments=False, \n",
    "    limit=20,\n",
    "    max_pages=1000,\n",
    ")\n",
    "\n",
    "print(confluence_documents[1])\n",
    "print(f'{len(confluence_documents)} documents read from Confluence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are a lot of changes made here throug changing the function names and behavior in openai ([Migration Guide](https://github.com/openai/openai-python/discussions/742)). Also I didn't use the suggested summary function, but I fixed it too, si it's available for testing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import math\n",
    "import textwrap\n",
    "\n",
    "def get_token_size(document, model):\n",
    "    tokenizer=tiktoken.encoding_for_model(model)\n",
    "    return len(tokenizer.encode(document))\n",
    "\n",
    "def naive_chunker(document, chunk_size, model):\n",
    "    tokenizer=tiktoken.encoding_for_model(model)\n",
    "    document_tokens=tokenizer.encode(document)\n",
    "    document_size = len(document_tokens)\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, document_size, chunk_size):\n",
    "        chunk = document_tokens[i:i + chunk_size]\n",
    "        chunks.append(tokenizer.decode(chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def auto_chunker(document, max_chunk_size, model):\n",
    "    tokenizer = tiktoken.encoding_for_model(model)\n",
    "    document_tokens = tokenizer.encode(document)\n",
    "    document_size = len(document_tokens)\n",
    "    # total chunk number\n",
    "    K = math.ceil(document_size / max_chunk_size)\n",
    "    # average integer chunk size\n",
    "    average_chunk_size = math.ceil(document_size / K)\n",
    "    # number of chunks with average_chunk_size - 1 \n",
    "    shorter_chunk_number = K * average_chunk_size - document_size\n",
    "    # number of chunks with average_chunk_size\n",
    "    standard_chunk_number = K - shorter_chunk_number\n",
    "\n",
    "    chunks = []\n",
    "    chunk_start = 0\n",
    "    for i in range(0, K):\n",
    "        if i < standard_chunk_number:\n",
    "            chunk_end = chunk_start + average_chunk_size\n",
    "        else:\n",
    "            chunk_end = chunk_start + average_chunk_size - 1\n",
    "        chunk = document_tokens[chunk_start:chunk_end]\n",
    "        chunks.append(tokenizer.decode(chunk))\n",
    "        chunk_start = chunk_end\n",
    "\n",
    "    assert chunk_start == document_size\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "delimiter = \"####\"\n",
    "\n",
    "MAX_ATTEMPTS = 3\n",
    "MODEL='gpt-4'\n",
    "\n",
    "\n",
    "def ChatGPT_API(messages, openai_key, model):\n",
    "    client = AzureOpenAI(\n",
    "        api_key=\"YOUR_OPENAI_API_KEY\",\n",
    "        api_version=\"YOUR_OPENAI_API_VERSION\",\n",
    "        azure_endpoint=\"YOUR_AZURE_OPENAI_ENDPOINT\",\n",
    "        azure_deployment=\"YOUR_AZURE_OPENAI_LLM_DEPLOYMENT_NAME\",\n",
    "    )\n",
    "    for attempt in range(MAX_ATTEMPTS):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "            )\n",
    "            break\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            time.sleep(1)\n",
    "            if attempt == MAX_ATTEMPTS - 1:\n",
    "                return \"Server Error\"\n",
    "            continue\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def get_chunk_summary(content, openai_key, model):\n",
    "    system_msg = f\"\"\"\n",
    "            Summarize this document chunk.\n",
    "            reply format：{delimiter}<summary>\"\"\"\n",
    "    user_msg = 'here is the document chunk:\\n' + content\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_msg}\n",
    "    ]\n",
    "    result = ChatGPT_API( messages, openai_key, model)\n",
    "    # print(result.split(delimiter)[-1].strip())\n",
    "    return result.split(delimiter)[-1].strip()\n",
    "\n",
    "def get_global_summary(list_of_summaries, openai_key, model):\n",
    "    \n",
    "    system_msg = f\"\"\"\n",
    "            You are given a a list of summaries, each summary summarizes a chunk of a document in sequence.\n",
    "            Combine a list of summaries into one global summary of the document.\n",
    "            reply format：{delimiter}<global summary>\"\"\"\n",
    "    user_msg = 'here is the list of the summaries:\\n' + str(list_of_summaries)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_msg}\n",
    "    ]\n",
    "        \n",
    "    result = ChatGPT_API(messages, openai_key, model)\n",
    "    # print(result)\n",
    "    return result.split(delimiter)[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE=1024\n",
    "test_document=' '.join([str(item) for item in confluence_documents])\n",
    "naive_chunks=naive_chunker(test_document, CHUNK_SIZE, AZURE_OPENAI_LLM_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_chunk_summaries=[get_chunk_summary(chunk,  OPENAI_API_KEY, AZURE_OPENAI_LLM_MODEL_NAME) for chunk in naive_chunks]\n",
    "naive_global_summary=get_global_summary(naive_chunk_summaries,OPENAI_API_KEY, AZURE_OPENAI_LLM_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print chunk sizes\n",
    "print('Chunk sizes:', [get_token_size(chunk, AZURE_OPENAI_LLM_MODEL_NAME) for chunk in naive_chunks])\n",
    "print('First chunks size:', [get_token_size(naive_chunks[0], AZURE_OPENAI_LLM_MODEL_NAME)])\n",
    "print('Last chunks size:', [get_token_size(naive_chunks[-1], AZURE_OPENAI_LLM_MODEL_NAME)])\n",
    "\n",
    "# Print chunk sizes\n",
    "print(len(naive_chunks),' chunks generated.')\n",
    "    \n",
    "# Print last chunk text\n",
    "print('Last chunk text:', naive_chunks[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_CHUNK_SIZE=1024\n",
    "auto_chunks=auto_chunker(test_document, MAX_CHUNK_SIZE, AZURE_OPENAI_LLM_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_chunk_summaries=[get_chunk_summary(chunk,  AZURE_OPENAI_LLM_MODEL_NAME, AZURE_OPENAI_LLM_MODEL_NAME) for chunk in auto_chunks]\n",
    "auto_global_summary=get_global_summary(auto_chunk_summaries,AZURE_OPENAI_LLM_MODEL_NAME, AZURE_OPENAI_LLM_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('chunk size list:',[get_token_size(chunk, MODEL) for chunk in auto_chunks])\n",
    "print('First chunks size:', [get_token_size(auto_chunks[0], AZURE_OPENAI_LLM_MODEL_NAME)])\n",
    "print('Last chunks size:', [get_token_size(auto_chunks[-1], AZURE_OPENAI_LLM_MODEL_NAME)])\n",
    "\n",
    "print(len(auto_chunks),' chunks generated.')\n",
    "    \n",
    "print('Last chunk text:', auto_chunks[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use for now\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# https://js.langchain.com/docs/modules/data_connection/document_transformers/#get-started-with-text-splitters\n",
    "# How to chunk: https://www.pinecone.io/learn/chunking-strategies/\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024, \n",
    "    chunk_overlap = 200,\n",
    "    length_function = len)\n",
    "\n",
    "splitted_documents = text_splitter.split_documents(confluence_documents)\n",
    "\n",
    "print(f'{len(splitted_documents)} chunks generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings by OpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# https://api.python.langchain.com/en/latest/embeddings/langchain_openai.embeddings.azure.AzureOpenAIEmbeddings.html\n",
    "openai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    # https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-preview\n",
    "    model=AZURE_OPENAI_EMBEDDING_MODEL_NAME,\n",
    "    embedding_ctx_length=8191, # default\n",
    "    chunk_size= 1024 # Shall be the same value (ENV) as for the text embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test naive_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.azure_cosmos_db import (\n",
    "    AzureCosmosDBVectorSearch,\n",
    "    CosmosDBSimilarityType,\n",
    ")\n",
    "from pymongo import MongoClient\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# LEARNINGS\n",
    "# 1) CosmosDB RU connection string needs to be pulled from Instance Connection String directly (not from the DB)\n",
    "# 2) Before connecting from local machine towards the DB make sure to open the firewall from Azure Portal (Networking)\n",
    "\n",
    "_indexName = 'km-index'\n",
    "_dbName, _collectionName = COSMOSDB_NAMESPACE.split(\".\")\n",
    "\n",
    "client: MongoClient = MongoClient(COSMOSDB_VCORE_CONNECTION_STRING)\n",
    "collection = client[_dbName][_collectionName]\n",
    "\n",
    "# Clean MongoDB collection before inserting new data\n",
    "collection.database.drop_collection(_collectionName)\n",
    "\n",
    "# Make Documents from naive_chunks, add \"source\" as a number of chunk for now\n",
    "naive_documents = [Document(page_content=chunk, metadata={\"source\": f\"chunk_{i}\"}) for i, chunk in enumerate(naive_chunks)]\n",
    "\n",
    "vectorstore = AzureCosmosDBVectorSearch.from_documents(\n",
    "    naive_documents,\n",
    "    openai_embeddings,\n",
    "    collection=collection,\n",
    "    index_name=_indexName,\n",
    ")\n",
    "\n",
    "num_lists = 100\n",
    "dimensions = 1536\n",
    "similarity_algorithm = CosmosDBSimilarityType.COS\n",
    "\n",
    "vectorstore.create_index(num_lists, dimensions, similarity_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "# Setup of the LLM and the chain\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=AZURE_OPENAI_LLM_DEPLOYMENT_NAME, \n",
    "    model=AZURE_OPENAI_LLM_MODEL_NAME,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "chain = load_qa_with_sources_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\")\n",
    "\n",
    "query = \"YOUR_EXAMPLE_QUESTION\"\n",
    "query1 = \"YOUR_EXAMPLE_QUESTION\"\n",
    "query2 = \"YOUR_EXAMPLE_QUESTION\"\n",
    "query3 = \"YOUR_EXAMPLE_QUESTION\"\n",
    "\n",
    "matching_docs = vectorstore.similarity_search(query)\n",
    "matching_docs1 = vectorstore.similarity_search(query1)\n",
    "matching_docs2 = vectorstore.similarity_search(query2)\n",
    "matching_docs3 = vectorstore.similarity_search(query3)\n",
    "\n",
    "answer = chain.run(input_documents=matching_docs, question=query)\n",
    "answer1 = chain.run(input_documents=matching_docs1, question=query1)\n",
    "answer2 = chain.run(input_documents=matching_docs2, question=query2)\n",
    "answer3 = chain.run(input_documents=matching_docs3, question=query3)\n",
    "\n",
    "print(query, \"\\n\", answer)\n",
    "print(query1, \"\\n\", answer1)\n",
    "print(query2, \"\\n\", answer2)\n",
    "print(query3, \"\\n\", answer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test auto_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.azure_cosmos_db import (\n",
    "    AzureCosmosDBVectorSearch,\n",
    "    CosmosDBSimilarityType,\n",
    ")\n",
    "from pymongo import MongoClient\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# LEARNINGS\n",
    "# 1) CosmosDB RU connection string needs to be pulled from Instance Connection String directly (not from the DB)\n",
    "# 2) Before connecting from local machine towards the DB make sure to open the firewall from Azure Portal (Networking)\n",
    "\n",
    "_indexName = 'km-index'\n",
    "_dbName, _collectionName = COSMOSDB_NAMESPACE.split(\".\")\n",
    "\n",
    "client: MongoClient = MongoClient(COSMOSDB_VCORE_CONNECTION_STRING)\n",
    "collection = client[_dbName][_collectionName]\n",
    "\n",
    "# Clean MongoDB collection before inserting new data\n",
    "collection.database.drop_collection(_collectionName)\n",
    "\n",
    "# Make Documents from auto_chunks, add \"source\" as a number of chunk for now\n",
    "auto_documents = [Document(page_content=chunk, metadata={\"source\": f\"chunk_{i}\"}) for i, chunk in enumerate(auto_chunks)]\n",
    "\n",
    "vectorstore = AzureCosmosDBVectorSearch.from_documents(\n",
    "    auto_documents,\n",
    "    openai_embeddings,\n",
    "    collection=collection,\n",
    "    index_name=_indexName,\n",
    ")\n",
    "\n",
    "num_lists = 100\n",
    "dimensions = 1536\n",
    "similarity_algorithm = CosmosDBSimilarityType.COS\n",
    "\n",
    "vectorstore.create_index(num_lists, dimensions, similarity_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "# Setup of the LLM and the chain\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=AZURE_OPENAI_LLM_DEPLOYMENT_NAME, \n",
    "    model=AZURE_OPENAI_LLM_MODEL_NAME,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "chain = load_qa_with_sources_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\")\n",
    "\n",
    "query = \"YOUR_EXAMPLE_QUESTION\"\n",
    "query1 = \"YOUR_EXAMPLE_QUESTION\"\n",
    "query2 = \"YOUR_EXAMPLE_QUESTION\"\n",
    "query3 = \"YOUR_EXAMPLE_QUESTION\"\n",
    "\n",
    "matching_docs = vectorstore.similarity_search(query)\n",
    "matching_docs1 = vectorstore.similarity_search(query1)\n",
    "matching_docs2 = vectorstore.similarity_search(query2)\n",
    "matching_docs3 = vectorstore.similarity_search(query3)\n",
    "\n",
    "answer = chain.run(input_documents=matching_docs, question=query)\n",
    "answer1 = chain.run(input_documents=matching_docs1, question=query1)\n",
    "answer2 = chain.run(input_documents=matching_docs2, question=query2)\n",
    "answer3 = chain.run(input_documents=matching_docs3, question=query3)\n",
    "\n",
    "print(query, \"\\n\", answer)\n",
    "print(query1, \"\\n\", answer1)\n",
    "print(query2, \"\\n\", answer2)\n",
    "print(query3, \"\\n\", answer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In this experiment, we investigated the impact of different chunking strategies on large document summarization using OpenAI's language models. I explored two approaches:\n",
    "\n",
    "*Naive Chunking:* A simple method that splits the document into fixed-size chunks. Our document resulted in 665 chunks of 1024 tokens each, with a final chunk of 138 tokens.\n",
    "*Auto Chunking:* A more sophisticated approach that aims to divide the document into chunks of approximately equal size, up to a maximum chunk size (1024 tokens in our case). This resulted in all chunks being very close to the maximum size.\n",
    "\n",
    "### Key Observations:\n",
    "Unequal vs. Equal Chunk Sizes: The naive approach led to a much smaller final chunk, potentially containing less information. Auto chunking, in contrast, ensured more balanced chunk sizes.\n",
    "Similar Summarization Performance: Despite this difference, both methods yielded very similar answers to most of our questions, suggesting the model's ability to synthesize information from across the document.\n",
    "\n",
    "### Specific Example: \"How to Present Decision Outcome?\" from the last chunk\n",
    "\n",
    "I tested the models' understanding of the last section of the document by asking: \"How to present a decision outcome?\"  Interestingly, both chunking strategies produced comprehensive responses:\n",
    "\n",
    "##### Naive Chunking Response:\n",
    "\n",
    "*How to present decision outcome?*\n",
    "\n",
    "To present a decision outcome, the chosen option should be clearly stated along with the justification for the choice. The decision-making process involves considering various options along with their pros and cons. The chosen option should address the problem statement effectively and meet the key decision drivers. It is important to outline both the positive and negative consequences of the decision to provide a comprehensive overview. Additionally, links to any related decisions or further information should be included to offer a complete picture and support transparency.\n",
    "\n",
    "##### Auto Chunking Response:\n",
    "\n",
    "*How to present decision outcome?*\n",
    "\n",
    "To present a decision outcome, the following structure should be used:\n",
    "\n",
    "1. **Status**: Indicate whether the decision is proposed, rejected, accepted, or deprecated.\n",
    "2. **Deciders**: List everyone involved in making the decision.\n",
    "3. **Date**: Provide the date when the decision was last updated.\n",
    "4. **Technical Story**: Give a brief description or link to a ticket/issue URL that necessitated the decision.\n",
    "5. **Context and Problem Statement**: Describe the context and problem statement. This can be done in two to three sentences or in the form of a question.\n",
    "6. **Decision Drivers**: List the factors or concerns that influenced the decision.\n",
    "7. **Considered Options**: Enumerate the options that were considered.\n",
    "8. **Pros and Cons of the Options**: For each considered option, list the advantages and disadvantages.\n",
    "9. **Decision Outcome**: State the chosen option and provide justification for the selection, based on how it meets critical criteria or resolves key issues identified in the decision drivers.\n",
    "10. **Positive Consequences**: Highlight the expected benefits or positive outcomes of the chosen decision.\n",
    "11. **Negative Consequences**: Mention any potential downsides or negative outcomes that may arise.\n",
    "12. **Links**: Include any relevant links for further reference.\n",
    "\n",
    "This format ensures clarity, accountability, and traceability in decision-making processes by systematically outlining the problem, considered options, and the rationale behind the final decision.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While auto-chunking offers a more theoretically sound approach to dividing documents, our results indicate that naive chunking can still be effective for question-answering tasks, especially when the model can draw from a broader context. However, if the content within individual chunks is critical and cannot be inferred from elsewhere, the more consistent chunk sizes of auto-chunking might provide an advantage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
